\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx} % Required for inserting images
\usepackage{geometry}
\usepackage{bm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% methods: description of exact model and factorized variational approximation

\begin{document}

\section{Model and factorized approximation}

Let $\mathcal{V}, \mathcal{E}$ be the sets of nodes and edges in the ARG. Let $t_i$ be the age of the $i$th node measured as generations in the past. Let $\mathcal{S} \subset \mathcal{V}$ be the subset of nodes that are contemporary samples ($t_i = 0, i \in \mathcal{S}$); and $\mathcal{N} \equiv \mathcal{V} \setminus \mathcal{S}$ be the set of internal nodes and non-contemporary samples ($t_i > 0, i \in \mathcal{N}$). 

To simplify notation, assume that a given pair of nodes is connected by at most one edge (this assumption is unnecessary in practice) and use $ij \in \mathcal{E}$ to denote the edge connecting parent $i$ to child $j$. Each edge has two dimensions: time $l_{ij} = t_i - t_j$ and genomic span $s_{ij}$ measured in base pairs. If the per-generation, per-base probability of mutation is a constant $\mu \ll 1$, then the probability of $y_{ij}$ mutations occurring along the edge is Poisson, \[
p(y_{ij} | t_{i} - t_{j}) = \frac{(\mu s_{ij})^{y_{ij}}}{\Gamma(y_{ij} + 1)} (t_i - t_j)^{y_{ij}} \exp\{-\mu s_{ij} (t_i - t_j)\} \mathbb{I}[0 < t_j < t_i]. 
\]
Let $p(t_i | \eta_i)$ be a prior distribution on the age of the $i$th internal node with hyperparameters $\eta_i$. The joint distribution of ages and per-edge mutation counts is then,
\[
p(\bm t, \bm y | \bm \eta) = \prod_{i \in \mathcal{N}} p(t_i | \eta_i) \prod_{ij \in \mathcal{E}} p(y_{ij} | t_i - t_j)
\]
and the full posterior of node ages is $p(\bm t | \bm y, \bm \eta) = Z^{-1} p(\bm t, \bm y | \bm \eta)$ where $Z = \int p(\bm t, \bm y | \bm \eta) d\bm t$ is the marginal probability of the mutations.

Exact posterior inference of node ages is not tractable because $Z$ is a high-dimensional integral over an irregular polyhedron. Instead, we construct a variational approximation with the intent of accurately representing the marginal posterior distributions of node ages; that is, we use the fully-factorized approximation $q_{\bm \theta}(\bm t) \approx p(\bm t | \bm y, \bm \eta)$, where
\[
\begin{aligned}
q_{\bm \theta}(\bm t) & = \prod_{i \in \mathcal{N}} q_{\theta_i}(t_i) \\
q_{\theta_i}(t_i) & \approx p(t_i | \bm y, \bm \eta) = \int p(\bm t | \bm y, \bm \eta) d\bm t^{\setminus i}
\end{aligned}
\]
so that $q_{\theta_i}$ is the approximate marginal from a chosen parametric family (gamma) and $\theta$ are node-specific variational parameters. Thus, uncertainty in the ages of individual nodes is quantified in a manner that is much more concise than Monte Carlo samples (e.g. by storing two variational parameters rather than a large number of samples per node), at the expense of retaining information regarding the joint distribution of ages.

From a computational perspective, this approximation scheme converts the integration problem into an optimization problem: find the values of $\bm \theta$ that produce the best approximation according to some distributional measure of goodness-of-fit. For example, by minimizing the Kullback-Leiber divergence via moment matching of sufficient statistics (Appendix \ref{supp:kl-minimization}),
\begin{equation}
\label{eq:global-kl-objective}
\argmin_{\bm \theta} \int p(\bm t | \bm y , \bm \eta) \log \frac{p(\bm t | \bm y , \bm \eta)}{q_{\bm \theta}(\bm t)} d\bm t \implies \mathbb{E}_p[\phi(\bm t)] = \mathbb{E}_{q_{\bm \theta}}[\phi(\bm t)]
\end{equation}
where $\phi(\bm t) = \{(t_i, \log t_i)~\forall i \in \mathcal{N}\}$ are gamma sufficient statistics for each node.

\section{Expectation propagation}

However, the minimization in Equation \ref{eq:global-kl-objective} is still intractable, because computing $\mathbb{E}_p[\phi(\bm t)]$ also involves a high-dimensional integral. Instead, we use ``expectation propagation'' \cite{minka2002expectation}, which approximates the global optimization problem with an iterated series of local problems.

Because the variational marginals $q_{\theta_i}$ belong to the same exponential family, they may be arbitrarily factorized into
\[
q_{\theta_i}(t_i) \propto \exp\{ \phi(t_i)^T \theta_i \} = \prod_k \exp\{\phi(t_i)^T \theta_{i,k}  \}    
\]
where $\sum_k \theta_{i,k} = \theta_i$ Note that the individual factors have the same form as the original distribution -- e.g. are log-linear with the same sufficient statistics -- but are not required to have a finite integral (as long as their product can be normalized). Thus, \ref{eq:factorized-approximation} may be rewritten as,
\[
q_{\bm \theta}(\bm t) = \prod_{i \in \mathcal{N}} q_i(t_i) \propto \prod_{i \in \mathcal{N}} q_{i,0}(t_i) \prod_{ij \in \mathcal{E}} q_{i,ij}(t_i) q_{j,ij}(t_j)
\]
where we have used the shorthand $q_{i,k} \equiv q_{\theta_{i,k}}$. In words, the variational marginals for each node are split into ``messages'' that correspond to the prior and edges, mimicking the form of the unnormalized posterior. For the moment we assume that the prior for each node is a gamma distribution, so that $q_{i,0}(t_i) \propto p(t_i | \eta_i)$. 

Expectation propagation works by iteratively refining the messages, one at a time. For edge $ij$, the ``cavity'' distribution is the approximation without the $ij$-th factor,
\[
q^{\setminus ij}_{\bm \theta}(\bm t) \propto \frac{q_{\bm \theta}(\bm t)}{q_{i,ij}(t_i) q_{j,ij}(t_j)} \propto \exp\{ \phi(t_i)^T (\theta_i - \theta_{i,ij}) + \phi(t_j)^T (\theta_j - \theta_{j,ij})\} \prod_{k \in \mathcal{V} \setminus i,j} q_k(t_k)
\]
The ``target'' is the product of the cavity and the edge factor from the exact unnormalized posterior,
\[
q^{\setminus ij}_{\bm \theta} p_{ij} (\bm t) \propto q^{\setminus ij}_{\bm \theta}(\bm t) \times p(y_{ij} \mid t_i - t_j)
\]
Intuitively, the target is a surrogate for the posterior that is exact locally (e.g. for the edge being refined) and approximate elsewhere. The variational parameters are updated by matching moments against the target:
\[
\argmin_{\bm \theta'} \mathcal{D}( q^{\setminus ij}_{\bm \theta} p_{ij} \| q_{\bm \theta'} ) \implies \mathbb{E}_{q^{\setminus ij}_{\bm \theta} p_{ij}}[\phi(\bm t)] = \mathbb{E}_{q_{\bm \theta'}}[\phi(\bm t)]
\]
where the dependence on previous ($\bm \theta$) and updated ($\bm \theta'$) parameters is made explicit. The crucial difference with the global problem (\ref{eq:global-kl-objective}) is that the target is \emph{mostly factorized}, so that $\mathbb{E}_{q^{\setminus ij}_{\bm \theta} p_{ij}}[\phi(\bm t)]$ and $\mathbb{E}_{q_{\bm \theta'}}[\phi(\bm t)]$ only differ for $\phi(t_i)$ and $\phi(t_j)$. Thus, the only variational marginals that need modification are $q_{i}$ and $q_{j}$. The messages are updated by dividing the new approximate posterior by the cavity (equivalent to subtracting natural parameters), so that $\theta'_{i,ij} = \theta'_i - (\theta_i - \theta_{i,ij})$ and $\theta'_{i,ij} = \theta'_i - (\theta_j - \theta_{j,ij})$. 

What remains is to calculate expectations of gamma sufficient statistics under the target. At this point, it is useful to switch to the typical gamma parametrization, such that $q_{i}(t_i) \propto \exp \{ -t_i \beta_i + \log t_i (\alpha_i - 1) \}$. Then, the normalizer of the target is,
\[
\begin{aligned}
Z_{ij} & = \int q^{\setminus ij}_{\bm \theta}(\bm t) p(y_{ij} | t_i - t_j) d\bm t \\
& = C \times \frac{\mathcal{B}(y_{ij} + 1, \alpha_j) \Gamma(\alpha_i + \alpha_j + y_{ij})}{(\mu s_{ij} + \beta_i)^{\alpha_i + \alpha_j + y_{ij}}} {_2}F{_1}\left( {{\alpha_j,  \alpha_i + \alpha_j + y_{ij} } \atop {\alpha_j + y_{ij} + 1}}; \frac{\mu s_{ij} - \beta_j}{\mu s_{ij} + \beta_i} \right)
\end{aligned}
\]
where ${_2}F{_1}$ is the Gaussian hypergeometric function and $C$ is an irrelevant term that depends on the unmodified variational parameters. If node $j$ is a contemporary sample ($t_j = 0$) this simplifies to $Z_{ij} = C \times \Gamma(\alpha_i + y_{ij}) (\mu s_{ij} + \beta_i)^{-\alpha_i - y_{ij}}$. The derivation is given in Appendix \ref{supp:2f1-proofs}. Because the target is also an exponential family (albeit an atypical one), the expected sufficient statistics are
\[
\begin{aligned}
\mathbb{E}_{q^{\setminus ij}_{\bm \theta} p_{ij}}[t_i] = -\frac{\partial \log Z_{ij}}{\partial \beta_i},~~~ \mathbb{E}_{q^{\setminus ij}_{\bm \theta} p_{ij}}[\log t_i] = \frac{\partial \log Z_{ij}}{\partial \alpha_i} \\
\mathbb{E}_{q^{\setminus ij}_{\bm \theta} p_{ij}}[t_j] = -\frac{\partial \log Z_{ij}}{\partial \beta_j},~~~ \mathbb{E}_{q^{\setminus ij}_{\bm \theta} p_{ij}}[\log t_j] = \frac{\partial \log Z_{ij}}{\partial \alpha_j} \\
\end{aligned}
\] 
The partial derivatives may be efficiently calculated by taking a Taylor expansion of ${_2}F{_1}$ and differentiating term-by-term (Appendix \ref{supp:2f1-computation}). Once moments are in hand, Newton's method is used to find the corresponding values of $\theta'_i, \theta'_j$ (Appendix \ref{supp:gamma-kl-min}).

Expectation propagation converges to a fixed point that is often very close to the global minimum (\ref{eq:global-kl-objective}) \cite{cunningham2014gaussian}, and may be parallelized over edges (e.g. by using the same initial $\bm \theta$ for each edge update, then summing messages to get $\bm \theta'$).






\end{document}